{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "united-composite",
   "metadata": {},
   "source": [
    "# Apprentice Chef\n",
    "\n",
    "### A1: Classification_Analysis\n",
    "Estrella Spaans | Machine Learning\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "Apprentice Chef, Inc. has launched Halfway There, a cross-selling promotion where subscribers receive a half bottle of wine from a local California vineyard every Wednesday (halfway through the work week). The executives at Apprentice Chef also believe this endeavor will create a competitive advantage based on its unique product offering of hard to find local wines from smaller vineyards.\n",
    "\n",
    "Halfway There has been exclusively offered to all of the customers in the dataset you received, and the executives would like to promote this service to a wider audience. They have tasked you with analyzing their data, developing your top insights, and building a machine learning model to predict which customers will subscribe to this service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-experience",
   "metadata": {},
   "source": [
    "## 1. Data Preperation\n",
    "\n",
    "In here, I imported the essential packages and uploaded the file as a dataframe. In the description, it was mentioned that one of th columns was mislabeled, so I changed 'LARGEST_ORDER_SIZE' to 'AVERAGE_MEALS_ORDERED'. I also changed the columns names to lowercase as this is easier for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ethical-westminster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:24.733386Z",
     "start_time": "2021-02-15T21:45:22.383713Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>cross_sell_success</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>first_name</th>\n",
       "      <th>family_name</th>\n",
       "      <th>total_meals_ordered</th>\n",
       "      <th>unique_meals_purch</th>\n",
       "      <th>contacts_w_customer_service</th>\n",
       "      <th>product_categories_viewed</th>\n",
       "      <th>avg_time_per_site_visit</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>cancellations_before_noon</th>\n",
       "      <th>cancellations_after_noon</th>\n",
       "      <th>tastes_and_preferences</th>\n",
       "      <th>pc_logins</th>\n",
       "      <th>mobile_logins</th>\n",
       "      <th>weekly_plan</th>\n",
       "      <th>early_deliveries</th>\n",
       "      <th>late_deliveries</th>\n",
       "      <th>package_locker</th>\n",
       "      <th>refrigerated_locker</th>\n",
       "      <th>avg_prep_vid_time</th>\n",
       "      <th>average_meals_ordered</th>\n",
       "      <th>master_classes_attended</th>\n",
       "      <th>median_meal_rating</th>\n",
       "      <th>avg_clicks_per_visit</th>\n",
       "      <th>total_photos_viewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Saathos</td>\n",
       "      <td>saathos@unitedhealth.com</td>\n",
       "      <td>Saathos</td>\n",
       "      <td>Saathos</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Alysanne Osgrey</td>\n",
       "      <td>alysanne.osgrey@ge.org</td>\n",
       "      <td>Alysanne</td>\n",
       "      <td>Osgrey</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>40.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Edwyd Fossoway</td>\n",
       "      <td>edwyd.fossoway@jnj.com</td>\n",
       "      <td>Edwyd</td>\n",
       "      <td>Fossoway</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>19.77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Eleyna Westerling</td>\n",
       "      <td>eleyna.westerling@ge.org</td>\n",
       "      <td>Eleyna</td>\n",
       "      <td>Westerling</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1490.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Elyn Norridge</td>\n",
       "      <td>elyn.norridge@jnj.com</td>\n",
       "      <td>Elyn</td>\n",
       "      <td>Norridge</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>40.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   revenue  cross_sell_success               name                     email first_name family_name  total_meals_ordered  unique_meals_purch  contacts_w_customer_service  product_categories_viewed  avg_time_per_site_visit  mobile_number  cancellations_before_noon  cancellations_after_noon  tastes_and_preferences  pc_logins  mobile_logins  weekly_plan  early_deliveries  late_deliveries  package_locker  refrigerated_locker  avg_prep_vid_time  average_meals_ordered  master_classes_attended  median_meal_rating  avg_clicks_per_visit  total_photos_viewed\n",
       "0    393.0                   1            Saathos  saathos@unitedhealth.com    Saathos     Saathos                   14                   6                           12                         10                    48.00              1                          3                         1                       1          5              2            0                 0                2               0                    0               33.4                      1                        0                   1                    17                    0\n",
       "1   1365.0                   1    Alysanne Osgrey    alysanne.osgrey@ge.org   Alysanne      Osgrey                   87                   3                            8                          8                    40.35              1                          0                         0                       1          5              1           12                 0                2               0                    0               84.8                      1                        0                   3                    13                  170\n",
       "2    800.0                   1     Edwyd Fossoway    edwyd.fossoway@jnj.com      Edwyd    Fossoway                   15                   7                           11                          5                    19.77              1                          3                         0                       1          6              1            1                 0                1               0                    0               63.0                      1                        0                   2                    16                    0\n",
       "3    600.0                   1  Eleyna Westerling  eleyna.westerling@ge.org     Eleyna  Westerling                   13                   6                           11                          5                    90.00              1                          2                         0                       1          6              1           14                 0                3               0                    0               43.8                      1                        0                   2                    14                    0\n",
       "4   1490.0                   1      Elyn Norridge     elyn.norridge@jnj.com       Elyn    Norridge                   47                   8                            6                         10                    40.38              1                          0                         0                       0          5              1            5                 0                8               0                    0               84.8                      1                        1                   3                    12                  205"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PACKAGES, FILE, CHANGES AND SHOWING DATA \n",
    "#1. Managing Tine\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#2. IMPORTANT PACKAGES\n",
    "import pandas as pd                                    # essential datascience\n",
    "import matplotlib.pyplot as plt                        # data visualization\n",
    "import numpy as np                                     # mathimatics\n",
    "import seaborn as sns                                  # enhanced graphics\n",
    "import sklearn.linear_model                            # Different models\n",
    "import statsmodels.formula.api as smf                  # statsmodels\n",
    "from sklearn.model_selection import train_test_split   # train/test split\n",
    "from sklearn.preprocessing import StandardScaler       # standard scaler\n",
    "from sklearn.metrics import confusion_matrix           # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score              # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier     # KNN Classification\n",
    "from sklearn.tree import DecisionTreeClassifier        # classification trees\n",
    "from sklearn.tree import export_graphviz               # exports graphics\n",
    "from six import StringIO                               # objects in memory\n",
    "from IPython.display import Image                      # displays on frontend\n",
    "import pydotplus                                       # Graphvizâ€™s Interface\n",
    "from sklearn.linear_model import LogisticRegression    # logistic regression\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier    # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier# gbm\n",
    "\n",
    "\n",
    "# pip install gender_guesser (remove # if you need to install it)\n",
    "import gender_guesser.detector as gender # guess gender based on (given) name\n",
    "\n",
    "# setting pandas print options (columns, rows, and display width)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n",
    "## 3. IMPORTING THE DATESET\n",
    "# Specifying the path and file name\n",
    "file = './datasets/Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# Reading the file into Python\n",
    "ap_customers = pd.read_excel(io=file)\n",
    "\n",
    "## 4. CHANGING MISLABELD COLUMN NAME AND COLUMN PRESENTATION\n",
    "# Changing the name of largest_order_size to average_meals_ordered\n",
    "ap_customers.rename(columns={'LARGEST_ORDER_SIZE':'AVERAGE_MEALS_ORDERED'}, \n",
    "                    inplace=True)\n",
    "\n",
    "# Changing the capitalized columns to lowercase (personal preference)\n",
    "ap_customers.columns = map(str.lower, ap_customers.columns)\n",
    "\n",
    "## 5. SHOWING THE DATAFRAME\n",
    "# Checking if the data was imported and changed correctly \n",
    "ap_customers.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-driving",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## 2. Understanding the Data\n",
    "\n",
    "In the exploratory analysis, I made sure to get familiar with the data, checking the data types, null-values, definitions so that I can catogize them into continious, count/interval, and categorical variable types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-zimbabwe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:24.736671Z",
     "start_time": "2021-02-15T21:45:24.735009Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1946 entries, 0 to 1945\n",
      "Data columns (total 28 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   revenue                      1946 non-null   float64\n",
      " 1   cross_sell_success           1946 non-null   int64  \n",
      " 2   name                         1946 non-null   object \n",
      " 3   email                        1946 non-null   object \n",
      " 4   first_name                   1946 non-null   object \n",
      " 5   family_name                  1899 non-null   object \n",
      " 6   total_meals_ordered          1946 non-null   int64  \n",
      " 7   unique_meals_purch           1946 non-null   int64  \n",
      " 8   contacts_w_customer_service  1946 non-null   int64  \n",
      " 9   product_categories_viewed    1946 non-null   int64  \n",
      " 10  avg_time_per_site_visit      1946 non-null   float64\n",
      " 11  mobile_number                1946 non-null   int64  \n",
      " 12  cancellations_before_noon    1946 non-null   int64  \n",
      " 13  cancellations_after_noon     1946 non-null   int64  \n",
      " 14  tastes_and_preferences       1946 non-null   int64  \n",
      " 15  pc_logins                    1946 non-null   int64  \n",
      " 16  mobile_logins                1946 non-null   int64  \n",
      " 17  weekly_plan                  1946 non-null   int64  \n",
      " 18  early_deliveries             1946 non-null   int64  \n",
      " 19  late_deliveries              1946 non-null   int64  \n",
      " 20  package_locker               1946 non-null   int64  \n",
      " 21  refrigerated_locker          1946 non-null   int64  \n",
      " 22  avg_prep_vid_time            1946 non-null   float64\n",
      " 23  average_meals_ordered        1946 non-null   int64  \n",
      " 24  master_classes_attended      1946 non-null   int64  \n",
      " 25  median_meal_rating           1946 non-null   int64  \n",
      " 26  avg_clicks_per_visit         1946 non-null   int64  \n",
      " 27  total_photos_viewed          1946 non-null   int64  \n",
      "dtypes: float64(3), int64(21), object(4)\n",
      "memory usage: 425.8+ KB\n"
     ]
    }
   ],
   "source": [
    "## Info & Descriptive Analysis \n",
    "\n",
    "## The number of non-null values / data type of each variable (remove # to run)\n",
    "ap_customers.info()\n",
    "\n",
    "## The descriptive statistics of numeric variables (remove # to run)\n",
    " #ap_customers.describe(include = 'number').round(decimals=2)\n",
    "\n",
    "## The descriptive statistics of non-numeric variables (remove # to run)\n",
    "# ap_customers.describe(include = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-galaxy",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /> \n",
    "\n",
    "Due to the fact that Apprentice Chef want to know what kind of customer would enroll into the program, <b> the response variable will be cross_sell_success</b>. Based on the outputs above, I identified the data type of each original variable in the dataset: \n",
    "\n",
    "| Continuous                | Count/Interval              | Categorical  \n",
    "|:-------------------------:|:---------------------------:|:----------------------:|\n",
    "| revenue                   | avg_clicks_per_visit        | cross_sell_success     | \n",
    "| avg_time_per_site_visit   | unique_meals_purch          | name                   |\n",
    "| avg_prep_vid_time         | contacts_w_customer_service | email                  |\n",
    "|                           | product_categories_viewed   | first_name             |\n",
    "|                           | cancellations_before_noon   | family_name            |\n",
    "|                           | cancellations_after_noon    | mobile_number          |\n",
    "|                           | pc_logins\t                  | tastes_and_preferences |\n",
    "|                           | mobile_logins               | package_locker         |\n",
    "|                           | weekly_plan                 | refrigerated_locker    |\n",
    "|                           | early_deliveries            |                        |\n",
    "|                           | late_deliveries             |                        |\n",
    "|                           | master_classes_attended     |                        |\n",
    "|                           | average_meals_ordered       |                        |\n",
    "|                           | total_meals_ordered         |                        |\n",
    "|                           | total_photos_viewed         |                        |\n",
    "|                           | median_meals_rating         |                        |\n",
    "|<img width=300/>|<img width=300/>|<img width=300/>|\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /> \n",
    "\n",
    "## 3. Preperation Feature Engineering\n",
    "\n",
    "<b> Functions & Variables </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pleased-webcam",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:24.741378Z",
     "start_time": "2021-02-15T21:45:24.738519Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Creating Specific placeholders for each of the variables \n",
    "\n",
    "# Creating an placeholder for delivery variables \n",
    "delivery_variables = ['early_deliveries', 'late_deliveries']\n",
    "\n",
    "# Creating an placeholder for cancellations variables \n",
    "cancellation_variables = ['cancellations_before_noon','cancellations_after_noon']\n",
    "\n",
    "# Creating an placeholder for behavior_variables\n",
    "behavior_variables = ['pc_logins', 'mobile_logins','avg_clicks_per_visit','product_categories_viewed', 'total_photos_viewed', 'median_meal_rating']\n",
    "\n",
    "# Creating an placeholder for service_variables\n",
    "purchase_variables = ['unique_meals_purch','average_meals_ordered','weekly_plan', \"total_meals_ordered\"]\n",
    "\n",
    "# Creating an placeholder for service_variables\n",
    "service_variables = ['contacts_w_customer_service','master_classes_attended']\n",
    "\n",
    "# Creating an placeholder for categorical_variables\n",
    "categorical_variables = ['name','email', 'first_name','family_name','mobile_number','cross_sell_success','tastes_and_preferences','package_locker', 'refrigerated_locker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mathematical-peter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:24.745360Z",
     "start_time": "2021-02-15T21:45:24.743113Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Copy the dataset to make changes with other variables \n",
    "\n",
    "# Make a copy\n",
    "ap_customer_2 = ap_customers.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-scheduling",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /> \n",
    "\n",
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appreciated-nigeria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.752545Z",
     "start_time": "2021-02-15T21:45:24.746894Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGIGEERING: \n",
    "\n",
    "##############################################################################\n",
    "# OCCASION\n",
    "\n",
    "# STEP 1: splitting personal emails\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in ap_customer_2.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = ap_customer_2.loc[index, 'email'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame to convert the email\n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "# Creating a new list\n",
    "placeholder_lst2 = []\n",
    "\n",
    "#defining which emails belong to professional \n",
    "professional = ['mmm.com','amex.com','apple.com','boeing.com','caterpillar.com',\\\n",
    "                'chevron.com','cisco.com','cocacola.com','disney.com','dupont.com',\\\n",
    "                'exxon.com','ge.org','goldmansacs.com','homedepot.com','ibm.com',\\\n",
    "                'intel.com@jnj.com','jpmorgan.com','mcdonalds.com','merck.com',\\\n",
    "                'microsoft.com','nike.com','pfizer.com','pg.com','travelers.com',\\\n",
    "                'unitedtech.com','unitedhealth.com','verizon.com','visa.com',\\\n",
    "                'walmart.com']\n",
    "\n",
    "#defining which emails belong to personal\n",
    "personal = ['gmail.com','yahoo.com','protonmail.com']\n",
    "\n",
    "# loop over each variable to determine which category it is and put it in\n",
    "# a list\n",
    "for row in email_df[1]:\n",
    "    if row in professional: \n",
    "        placeholder_lst2.append('work')\n",
    "    elif row in personal:\n",
    "        placeholder_lst2.append('personal')\n",
    "    else: \n",
    "        placeholder_lst2.append('other')\n",
    "\n",
    "# Adding a new column to the dataframe \n",
    "ap_customer_2['occasion'] = placeholder_lst2  \n",
    "\n",
    "# Setting a placeholder\n",
    "occasion = ['occasion']\n",
    "\n",
    "# Getting the dummies for each of the occasions. \n",
    "occasion_dummies = pd.get_dummies(ap_customer_2['occasion'])\n",
    "\n",
    "\n",
    "# Dropping the original column\n",
    "ap_customer_2 = ap_customer_2.drop(columns=['occasion'])\n",
    "\n",
    "# Adding the dummies to the feature engieering dataset\n",
    "ap_customer_2['other']= occasion_dummies.loc[:,'other']\n",
    "ap_customer_2['work']= occasion_dummies.loc[:,'work']\n",
    "ap_customer_2['personal']= occasion_dummies.loc[:,'personal']\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#GENDER\n",
    "# guessing gender based on (given) name\n",
    "def gender_guesser():\n",
    "    # Setting a placeholder for the list/ \n",
    "    placeholder_lst = []\n",
    "    # looping to guess gender\n",
    "    for name in ap_customer_2['first_name']:\n",
    "        guess = gender.Detector().get_gender(name)\n",
    "        print(guess)\n",
    "        placeholder_lst.append(guess)\n",
    "\n",
    "# Calling genderguesser to generate variables (remove # to run)\n",
    "# gender_guesser()\n",
    "\n",
    "# All variables copied from Gender Guesser\n",
    "gender_guess = np.array(['unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'mostly_male', 'female', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'male', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'male', 'unknown', 'female', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'female', 'female', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'mostly_male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'mostly_male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'mostly_male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'mostly_male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'andy', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'andy', 'male', 'unknown', 'unknown', 'male', 'male', 'female', 'female', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'female', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'mostly_female', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'mostly_female', 'unknown', 'male', 'unknown', 'female', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'female', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'male', 'mostly_male', 'male', 'male', 'male', 'male', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'andy', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'female', 'male', 'male', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'female', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'andy', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'mostly_male', 'male', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'mostly_male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'male', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'female', 'male', 'male', 'unknown', 'male', 'unknown', 'mostly_female', 'male', 'unknown', 'unknown', 'female', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'female', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'female', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'mostly_male', 'mostly_male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'female', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'male', 'male', 'mostly_male', 'unknown', 'unknown', 'male', 'andy', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'male', 'female', 'mostly_female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'mostly_female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'female', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'male', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'mostly_female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'mostly_female', 'female', 'female', 'male', 'male', 'male', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'female', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'andy', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'mostly_male', 'male', 'male', 'unknown', 'male', 'unknown', 'mostly_male', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'female', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'mostly_male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'andy', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'unknown'])\n",
    "\n",
    "# Changing some of the variables \n",
    "gender_guess[gender_guess == 'mostly_male'] = 'male'\n",
    "gender_guess[gender_guess == 'mostly_female'] = 'female'\n",
    "gender_guess[gender_guess == 'andy'] = 'unknown'\n",
    "\n",
    "# Adding the genders to the dataset\n",
    "ap_customer_2['gender_guess'] = pd.Series(gender_guess)\n",
    "\n",
    "# Getting the dummies for each of the genders \n",
    "gender_dummies = pd.get_dummies(ap_customer_2['gender_guess'])\n",
    "\n",
    "# Dropping the original column\n",
    "ap_customer_2 = ap_customer_2.drop(columns= ['gender_guess'])\n",
    "\n",
    "# Adding the dummies to the feature engieering dataset\n",
    "ap_customer_2 = ap_customer_2.join([gender_dummies])\n",
    "\n",
    "##############################################################################\n",
    "# Creating a new variable: total order with a calculation of other variables\n",
    "ap_customer_2['total_orders'] = round(ap_customer_2['total_meals_ordered'] / ap_customer_2['average_meals_ordered'],2)\n",
    "\n",
    "##############################################################################\n",
    "# Creating a new variable: total logins with a calculation of other variables\n",
    "ap_customer_2['total_logins'] = (ap_customer_2['pc_logins'] + ap_customer_2['mobile_logins'])\n",
    "\n",
    "#############################################################################\n",
    "# AVERAGE TIME ON THE WEBSITE\n",
    "\n",
    "# Creating a column\n",
    "ap_customer_2['length_time_spent_website'] = 0\n",
    "\n",
    "# iterating over each original column to\n",
    "# change values in the new feature columns\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # people that spend more than 60 minutes on the website\n",
    "    if ap_customer_2.loc[index, 'avg_time_per_site_visit'] > 60:\n",
    "        ap_customer_2.loc[index, 'length_time_spent_website'] = 1\n",
    "\n",
    "\n",
    "  # people that spend more less than 60 minutes on the website\n",
    "    if ap_customer_2.loc[index, 'avg_time_per_site_visit'] <=60:\n",
    "        ap_customer_2.loc[index, 'length_time_spent_website'] = 0\n",
    "        \n",
    "# Getting the dummies for the avg time spent on the website\n",
    "length_website = pd.get_dummies(ap_customer_2['length_time_spent_website'])\n",
    "\n",
    "# Changing the column names \n",
    "length_website.columns = ['below_60_min_website_time', 'above_60_min_website_time']\n",
    "\n",
    "# adding the data to the dataset\n",
    "ap_customer_2['below_60_min_website_time'] = length_website.loc[:, \"below_60_min_website_time\"]\n",
    "ap_customer_2['above_60_min_website_time'] = length_website.loc[:, \"above_60_min_website_time\"]\n",
    "\n",
    "# dropping original column\n",
    "ap_customer_2.drop(columns=(\"length_time_spent_website\"))\n",
    "\n",
    "#############################################################################\n",
    "#Delivery variables \n",
    "\n",
    "ap_customer_2['has_early_deliveries']   = 0\n",
    "ap_customer_2['has_late_deliveries'] = 0\n",
    "\n",
    "# create new variable named HAVE_EARLY_DELIVERIES\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # has_early_deliveries\n",
    "    if ap_customer_2.loc[index, 'early_deliveries'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_early_deliveries'] = 1\n",
    "        \n",
    "      # has_late_deliveries \n",
    "    if ap_customer_2.loc[index, 'late_deliveries'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_late_deliveries'] = 1\n",
    "        \n",
    "###############################################################################\n",
    "# Locker Variables\n",
    "# Creating a new columns for lockers based on a calculation       \n",
    "ap_customer_2['total_lockers'] = ap_customer_2['package_locker'] + ap_customer_2['refrigerated_locker']\n",
    "ap_customer_2['locker']   = 0\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating locker value based on condition\n",
    "    if ap_customer_2.loc[index, 'total_lockers'] > 0:\n",
    "        ap_customer_2.loc[index, 'locker'] = 1\n",
    "        \n",
    "###############################################################################    \n",
    "# CANCELLATIONS\n",
    "# Total number of calculations\n",
    "ap_customer_2['total_cancellations'] = ap_customer_2['cancellations_before_noon'] + ap_customer_2['cancellations_after_noon']\n",
    "\n",
    "\n",
    "# creating an empty column\n",
    "ap_customer_2['has_cancellations']   = 0\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating cancellation  value based on condition\n",
    "    if ap_customer_2.loc[index, 'total_cancellations'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_cancellations'] = 1\n",
    "\n",
    "###############################################################################    \n",
    "# Clicks, who has more than 10 clicks? \n",
    "\n",
    "# creating an empty column  \n",
    "ap_customer_2['click_above_avg_10']   = 0\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating click value based on condition\n",
    "    if ap_customer_2.loc[index, 'avg_clicks_per_visit'] > 10:\n",
    "        ap_customer_2.loc[index, 'click_above_avg_10'] = 1\n",
    "\n",
    "###############################################################################  \n",
    "# Median Ranking \n",
    "\n",
    "# creating an empty column  \n",
    "ap_customer_2['high_low_ranking']   = 0\n",
    "\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating ranking value based on condition\n",
    "    if ap_customer_2.loc[index, 'median_meal_rating'] > 3:\n",
    "        ap_customer_2.loc[index, 'high_low_ranking'] = 1\n",
    "        \n",
    "###############################################################################          \n",
    "#Video prep time above 60min\n",
    "\n",
    "# creating an empty column  \n",
    "ap_customer_2['60_min_prep_time']   = 0\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating video value based on condition\n",
    "    if ap_customer_2.loc[index, 'avg_prep_vid_time'] > 60:\n",
    "        ap_customer_2.loc[index, '60_min_prep_time'] = 1\n",
    "        \n",
    "############################################################################### \n",
    "# Has attended Masterclasses\n",
    "\n",
    "# creating an empty column  \n",
    "ap_customer_2['has_master_class_attended']   = 0\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating has attended value based on condition\n",
    "    if ap_customer_2.loc[index, 'master_classes_attended'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_master_class_attended'] = 1\n",
    "\n",
    "############################################################################### \n",
    "# has early deliveries\n",
    "\n",
    "# creating an empty column  \n",
    "ap_customer_2['has_early_deliveries']   = 0\n",
    "\n",
    "#go through values in column\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating has early delivery value based on condition\n",
    "    if ap_customer_2.loc[index, 'early_deliveries'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_early_deliveries'] = 1\n",
    "\n",
    "#############################################################################\n",
    "# Behavior Variables \n",
    "\n",
    "# creating an empty columns\n",
    "ap_customer_2['has_total_photos_viewed']   = 0\n",
    "ap_customer_2['has_mobile_logins']         = 0\n",
    "\n",
    "#go through values in columns\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating has mobile logins  value based on condition\n",
    "    if ap_customer_2.loc[index, 'mobile_logins'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_mobile_logins'] = 1\n",
    "\n",
    "  # creating has photos viewed value based on condition\n",
    "    if ap_customer_2.loc[index, 'total_photos_viewed'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_total_photos_viewed'] = 1\n",
    "        \n",
    "#############################################################################\n",
    "# Purchase Variables \n",
    "\n",
    "# creating an empty columns\n",
    "ap_customer_2['has_weekly_plan']   = 0\n",
    "\n",
    "\n",
    "#go through values in columns\n",
    "for index, value in ap_customer_2.iterrows():\n",
    "    \n",
    "    # creating has weekly plan value based on condition\n",
    "    if ap_customer_2.loc[index, 'weekly_plan'] > 0:\n",
    "        ap_customer_2.loc[index, 'has_weekly_plan'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "normal-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.778189Z",
     "start_time": "2021-02-15T21:45:29.754612Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>cross_sell_success</th>\n",
       "      <th>total_meals_ordered</th>\n",
       "      <th>unique_meals_purch</th>\n",
       "      <th>contacts_w_customer_service</th>\n",
       "      <th>product_categories_viewed</th>\n",
       "      <th>avg_time_per_site_visit</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>cancellations_before_noon</th>\n",
       "      <th>cancellations_after_noon</th>\n",
       "      <th>tastes_and_preferences</th>\n",
       "      <th>pc_logins</th>\n",
       "      <th>mobile_logins</th>\n",
       "      <th>weekly_plan</th>\n",
       "      <th>early_deliveries</th>\n",
       "      <th>late_deliveries</th>\n",
       "      <th>package_locker</th>\n",
       "      <th>refrigerated_locker</th>\n",
       "      <th>avg_prep_vid_time</th>\n",
       "      <th>average_meals_ordered</th>\n",
       "      <th>master_classes_attended</th>\n",
       "      <th>median_meal_rating</th>\n",
       "      <th>avg_clicks_per_visit</th>\n",
       "      <th>total_photos_viewed</th>\n",
       "      <th>other</th>\n",
       "      <th>work</th>\n",
       "      <th>personal</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>unknown</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_logins</th>\n",
       "      <th>length_time_spent_website</th>\n",
       "      <th>below_60_min_website_time</th>\n",
       "      <th>above_60_min_website_time</th>\n",
       "      <th>has_early_deliveries</th>\n",
       "      <th>has_late_deliveries</th>\n",
       "      <th>total_lockers</th>\n",
       "      <th>locker</th>\n",
       "      <th>total_cancellations</th>\n",
       "      <th>has_cancellations</th>\n",
       "      <th>click_above_avg_10</th>\n",
       "      <th>high_low_ranking</th>\n",
       "      <th>60_min_prep_time</th>\n",
       "      <th>has_master_class_attended</th>\n",
       "      <th>has_total_photos_viewed</th>\n",
       "      <th>has_mobile_logins</th>\n",
       "      <th>has_weekly_plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>40.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>19.77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1490.0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>40.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   revenue  cross_sell_success  total_meals_ordered  unique_meals_purch  contacts_w_customer_service  product_categories_viewed  avg_time_per_site_visit  mobile_number  cancellations_before_noon  cancellations_after_noon  tastes_and_preferences  pc_logins  mobile_logins  weekly_plan  early_deliveries  late_deliveries  package_locker  refrigerated_locker  avg_prep_vid_time  average_meals_ordered  master_classes_attended  median_meal_rating  avg_clicks_per_visit  total_photos_viewed  other  work  personal  female  male  unknown  total_orders  total_logins  length_time_spent_website  below_60_min_website_time  above_60_min_website_time  has_early_deliveries  has_late_deliveries  total_lockers  locker  total_cancellations  has_cancellations  click_above_avg_10  high_low_ranking  60_min_prep_time  has_master_class_attended  has_total_photos_viewed  has_mobile_logins  has_weekly_plan\n",
       "0    393.0                   1                   14                   6                           12                         10                    48.00              1                          3                         1                       1          5              2            0                 0                2               0                    0               33.4                      1                        0                   1                    17                    0      0     1         0       0     0        1          14.0             7                          0                          1                          0                     0                    1              0       0                    4                  1                   1                 0                 0                          0                        0                  1                0\n",
       "1   1365.0                   1                   87                   3                            8                          8                    40.35              1                          0                         0                       1          5              1           12                 0                2               0                    0               84.8                      1                        0                   3                    13                  170      0     1         0       0     0        1          87.0             6                          0                          1                          0                     0                    1              0       0                    0                  0                   1                 0                 1                          0                        1                  1                1\n",
       "2    800.0                   1                   15                   7                           11                          5                    19.77              1                          3                         0                       1          6              1            1                 0                1               0                    0               63.0                      1                        0                   2                    16                    0      1     0         0       0     0        1          15.0             7                          0                          1                          0                     0                    1              0       0                    3                  1                   1                 0                 1                          0                        0                  1                1\n",
       "3    600.0                   1                   13                   6                           11                          5                    90.00              1                          2                         0                       1          6              1           14                 0                3               0                    0               43.8                      1                        0                   2                    14                    0      0     1         0       0     0        1          13.0             7                          1                          0                          1                     0                    1              0       0                    2                  1                   1                 0                 0                          0                        0                  1                1\n",
       "4   1490.0                   1                   47                   8                            6                         10                    40.38              1                          0                         0                       0          5              1            5                 0                8               0                    0               84.8                      1                        1                   3                    12                  205      1     0         0       0     0        1          47.0             6                          0                          1                          0                     0                    1              0       0                    0                  0                   1                 0                 1                          1                        1                  1                1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking_Final Dataset \n",
    "# dropping categorical variables after they've been encoded\n",
    "categorical_variables2 = ['name', 'first_name','email','family_name']\n",
    "\n",
    "# Cropping the columns that are not needed\n",
    "ap_customer_2 = ap_customer_2.drop(columns= categorical_variables2)\n",
    "\n",
    "ap_customer_2.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-petite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T02:30:07.039987Z",
     "start_time": "2021-02-15T02:30:07.036483Z"
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /> \n",
    "\n",
    "## 5. Model Development\n",
    "\n",
    "### Preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handmade-holmes",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.783022Z",
     "start_time": "2021-02-15T21:45:29.779439Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Variable Options \n",
    " \n",
    "options = {\n",
    "\n",
    " 'option1'   : ['mobile_number','cancellations_before_noon','pc_logins', 'mobile_logins','refrigerated_locker','work','personal','other','female','male','unknown','tastes_and_preferences','contacts_w_customer_service','product_categories_viewed','has_early_deliveries'],\n",
    "\n",
    " 'option2'   : ['contacts_w_customer_service','mobile_number', 'tastes_and_preferences','pc_logins','early_deliveries','total_cancellations','locker','60_min_prep_time','has_master_class_attended'], \n",
    "\n",
    " 'option3'   : ['mobile_number','cancellations_before_noon','total_logins','refrigerated_locker','personal','other','tastes_and_preferences','contacts_w_customer_service','product_categories_viewed','has_early_deliveries','female','male','total_orders','has_master_class_attended','60_min_prep_time','total_meals_ordered'],\n",
    "\n",
    " 'option4'   : ['revenue', 'total_meals_ordered', 'unique_meals_purch', 'contacts_w_customer_service', 'product_categories_viewed', 'avg_time_per_site_visit', 'mobile_number', 'tastes_and_preferences', 'pc_logins', 'mobile_logins', 'weekly_plan', 'late_deliveries', 'average_meals_ordered','total_photos_viewed', 'personal', 'female', 'has_cancellations', 'locker', 'click_above_avg_10', 'high_low_ranking', '60_min_prep_time', 'has_master_class_attended', 'has_early_deliveries']  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharing-recipient",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.792261Z",
     "start_time": "2021-02-15T21:45:29.786401Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Creating Performance Overview \n",
    "# Convertion variables to show\n",
    "values = options.values()\n",
    "values_list = list(values)\n",
    "\n",
    "#Creating a dataframe\n",
    "model_performance = pd.DataFrame()\n",
    "model_performance['Model Name'] = 0\n",
    "model_performance['AUC Score'] = 0\n",
    "model_performance['Training Accuracy'] = 0\n",
    "model_performance['Testing Accuracy'] = 0\n",
    "model_performance['Confusion Matrix'] = 0\n",
    "model_performance['Variable Option'] = 0\n",
    "model_performance['Variables'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-trigger",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impaired-recruitment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.850063Z",
     "start_time": "2021-02-15T21:45:29.794527Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Training ACCURACY: 0.7252\n",
      "Testing  ACCURACY: 0.7351\n",
      "ACCURACY GAP: -0.00990000000000002\n",
      "AUC Score: 0.6391\n",
      "\n",
      "Confusion Matrix\n",
      "True Negatives : 58\n",
      "False Positives: 98\n",
      "False Negatives: 31\n",
      "True Positives : 300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION Model  \n",
    "# train/test split with the full model\n",
    "lr_data   =  ap_customer_2.loc[ : , options['option1']]\n",
    "target  =  ap_customer_2.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            lr_data,\n",
    "            target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1.0,\n",
    "                            warm_start = False,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test, y_test).round(4) # accuracy\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)\n",
    "\n",
    "# SCORING the results\n",
    "print('Results:')\n",
    "print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(x_test, y_test).round(4))\n",
    "print('ACCURACY GAP:', logreg_train_score - logreg_test_score)\n",
    "print('AUC Score:',logreg_auc_score)\n",
    "\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "Confusion Matrix\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suffering-sessions",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.859059Z",
     "start_time": "2021-02-15T21:45:29.851748Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adding Models for overview \n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Logistic Regression',\n",
    "                           'Training Accuracy'  : logreg_train_score,\n",
    "                           'Testing Accuracy'   : logreg_test_score,\n",
    "                           'AUC Score'          : logreg_auc_score,\n",
    "                           'Confusion Matrix'   : (logreg_tn,\n",
    "                                                   logreg_fp,\n",
    "                                                   logreg_fn,\n",
    "                                                   logreg_tp),\n",
    "                           'Variable Option'    : 'option 1',\n",
    "                           'Variables'           : values_list[0]},ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-massachusetts",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "promotional-scott",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.865929Z",
     "start_time": "2021-02-15T21:45:29.860628Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FUNCTION: display_tree \n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width)\n",
    "    \n",
    "    return img\n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    model_sorted = np.sort(model.feature_importances_)\n",
    "    \n",
    "    plt.barh(range(n_features), model_sorted, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('./analysis_images/Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unknown-position",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:29.897249Z",
     "start_time": "2021-02-15T21:45:29.867699Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Full Tree Training ACCURACY: 0.7293\n",
      "Full Tree Testing ACCURACY : 0.6982\n",
      "Full ACCURACY GAP : 0.0311\n",
      "Full Tree AUC Score: 0.6119\n",
      "\n",
      "Confusion Matrix\n",
      "True Negatives : 58\n",
      "False Positives: 98\n",
      "False Negatives: 49\n",
      "True Positives : 282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DECISION TREE Model \n",
    "# train/test split with the full model\n",
    "decision_tree_data   =  ap_customer_2.loc[ : , options['option1']]\n",
    "target  =  ap_customer_2.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            decision_tree_data,\n",
    "            target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier(splitter = 'best', \n",
    "                                   min_samples_leaf = 3, \n",
    "                                   max_depth = 4, \n",
    "                                   criterion = 'entropy',\n",
    "                                   random_state = 219)\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc\n",
    "\n",
    "# SCORING the model\n",
    "print('Results:')\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                    y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                    y_test).round(4))\n",
    "print('Full ACCURACY GAP :', (full_tree_train_score-full_tree_test_score).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "Confusion Matrix\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fatty-partnership",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:31.914607Z",
     "start_time": "2021-02-15T21:45:29.899114Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \"\"\"\n",
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuned best estimors:\n",
      "\"Tuned Parameters  :\", {'splitter': 'random', 'min_samples_leaf': 13, 'max_depth': 8, 'criterion': 'entropy'}\n",
      "\"Tuned CV AUC      :\", 0.6362\n",
      "\n",
      "\n",
      "Tuned Decision Tree Results:\n",
      "'Training ACCURACY:', 0.743\n",
      "'Testing  ACCURACY:', 0.7556\n",
      "'ACCURACY GAP:',  -0.012600000000000056\n",
      "'AUC Score:', 0.6779 \n",
      "\n",
      "Tuned Decision Tree Confusion Matrix:\n",
      "True Negatives : 72\n",
      "False Positives: 84\n",
      "False Negatives: 35\n",
      "True Positives : 296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TUNED DECISION TREE  \n",
    "# declaring a hyperparameter space\n",
    "criterion_space = ['gini', 'entropy']\n",
    "splitter_space  = ['best', 'random']\n",
    "depth_space     = pd.np.arange(1, 9, 1)\n",
    "leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_space,\n",
    "              'splitter'         : splitter_space,\n",
    "              'max_depth'        : depth_space,\n",
    "              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                   param_distributions   = param_grid,\n",
    "                                   cv                    = 3,\n",
    "                                   n_iter                = 150,\n",
    "                                   random_state          = 219,\n",
    "                                   scoring = make_scorer(roc_auc_score,\n",
    "                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_tree_cv.fit(decision_tree_data, target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(f\"\"\"\n",
    "\n",
    "Tuned best estimors:\n",
    "\"Tuned Parameters  :\", {tuned_tree_cv.best_params_}\n",
    "\"Tuned CV AUC      :\", {tuned_tree_cv.best_score_.round(4)}\"\"\")\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = tuned_tree_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(x_test)\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "tree_tuned_tn, \\\n",
    "tree_tuned_fp, \\\n",
    "tree_tuned_fn, \\\n",
    "tree_tuned_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "Tuned Decision Tree Results:\n",
    "'Training ACCURACY:', {tree_tuned_train_score}\n",
    "'Testing  ACCURACY:', {tree_tuned_test_score}\n",
    "'ACCURACY GAP:',  {tree_tuned_train_score - tree_tuned_test_score}\n",
    "'AUC Score:', {tree_tuned_auc} \n",
    "\n",
    "Tuned Decision Tree Confusion Matrix:\n",
    "True Negatives : {tree_tuned_tn}\n",
    "False Positives: {tree_tuned_fp}\n",
    "False Negatives: {tree_tuned_fn}\n",
    "True Positives : {tree_tuned_tp}\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distinguished-jonathan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:31.928697Z",
     "start_time": "2021-02-15T21:45:31.917169Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adding Models for overview \n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Decision Tree',\n",
    "                           'Training Accuracy'  : full_tree_train_score,\n",
    "                           'Testing Accuracy'   : full_tree_test_score,\n",
    "                           'AUC Score'          : full_tree_auc_score,\n",
    "                           'Confusion Matrix'   : (full_tree_tn,\n",
    "                                                   full_tree_fp,\n",
    "                                                   full_tree_fn,\n",
    "                                                   full_tree_tp),\n",
    "                           'Variable Option'    : 'option 1',\n",
    "                           'Variables'           : values_list[0]},ignore_index = True)\n",
    "\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned Decision Tree',\n",
    "                           'Training Accuracy'  : tree_tuned_train_score,\n",
    "                           'Testing Accuracy'   : tree_tuned_test_score,\n",
    "                           'AUC Score'          : tree_tuned_auc,\n",
    "                           'Confusion Matrix'   : (tree_tuned_tn,\n",
    "                                                   tree_tuned_fp,\n",
    "                                                   tree_tuned_fn,\n",
    "                                                   tree_tuned_tp),\n",
    "                           'Variable Option'    : 'option 1',\n",
    "                           'Variables'           : values_list[0]},ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-national",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T02:37:48.941332Z",
     "start_time": "2021-02-15T02:37:48.929247Z"
    }
   },
   "source": [
    "### KNN Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tutorial-graduate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:42.690993Z",
     "start_time": "2021-02-15T21:45:31.931921Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuned best estimors:\n",
      "\"Tuned Parameters  :\", {'weights': 'uniform', 'p': 1, 'n_neighbors': 12, 'n_jobs': 1, 'leaf_size': 9}\n",
      "\"Tuned Training AUC:\", 0.6174\n",
      "\n",
      "\n",
      "Tuned KKN Non-Standardized Results:\n",
      "Training ACCURACY:  0.7512\n",
      "Testing  ACCURACY:  0.7639\n",
      "ACCURACY GAP:       -0.012700000000000045\n",
      "AUC Score:'         0.6119\n",
      "\n",
      "\n",
      "Tuned KNN Non-Standardized Confusion Matrix:\n",
      "True Negatives : 65\n",
      "False Positives: 91\n",
      "False Negatives: 24\n",
      "True Positives : 307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuned KNN Classification \n",
    "\n",
    "knn_data   =  ap_customer_2.loc[ : , options['option1']]\n",
    "target  =  ap_customer_2.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            knn_data,\n",
    "            target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = target)\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "leaf_size   = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p           = [1,2]\n",
    "weights     = ['uniform', 'distance']\n",
    "n_jobs      = [1,-1]\n",
    "\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'leaf_size'        : leaf_size,\n",
    "              'n_neighbors'      : n_neighbors,\n",
    "              'p'                : p,\n",
    "              'weights'          : weights,\n",
    "              'n_jobs'            : n_jobs}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_knn_cv = RandomizedSearchCV(estimator             = tuned_knn,\n",
    "                                  param_distributions   = param_grid,\n",
    "                                  cv                    = 4,\n",
    "                                  n_iter                = 100,\n",
    "                                  random_state          = 219,\n",
    "                                  scoring = make_scorer(roc_auc_score,\n",
    "                                                  needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_knn_cv.fit(knn_data, target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "Tuned best estimors:\n",
    "\"Tuned Parameters  :\", {tuned_knn_cv.best_params_}\n",
    "\"Tuned Training AUC:\", {tuned_knn_cv.best_score_.round(4)}\"\"\")\n",
    "\n",
    "# 6. TUNED KNN Neighbors Classisication Model \n",
    "knn_pruned = tuned_knn_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pruned_pred = knn_pruned.predict(x_test)\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_pruned_train_score = knn_pruned.score(x_train, y_train).round(4) # accuracy\n",
    "knn_pruned_test_score  = knn_pruned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "knn_pruned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = full_tree_pred).round(4) # auc\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "knn_tuned_tn, \\\n",
    "knn_tuned_fp, \\\n",
    "knn_tuned_fn, \\\n",
    "knn_tuned_tp = confusion_matrix(y_true = y_test, y_pred = knn_pruned_pred).ravel()\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "Tuned KKN Non-Standardized Results:\n",
    "Training ACCURACY:  {knn_pruned_train_score}\n",
    "Testing  ACCURACY:  {knn_pruned_test_score }\n",
    "ACCURACY GAP:       {knn_pruned_train_score - knn_pruned_test_score}\n",
    "AUC Score:'         {knn_pruned_auc}\n",
    "\n",
    "\n",
    "Tuned KNN Non-Standardized Confusion Matrix:\n",
    "True Negatives : {knn_tuned_tn}\n",
    "False Positives: {knn_tuned_fp}\n",
    "False Negatives: {knn_tuned_fn}\n",
    "True Positives : {knn_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "assigned-lighter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:42.700287Z",
     "start_time": "2021-02-15T21:45:42.693684Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adding Models for overview \n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'TUNED KNN Non Standardized',\n",
    "                           'Training Accuracy'  : knn_pruned_train_score,\n",
    "                           'Testing Accuracy'   : knn_pruned_test_score,\n",
    "                           'AUC Score'          : knn_pruned_auc,\n",
    "                           'Confusion Matrix'   : (knn_tuned_tn,\n",
    "                                                   knn_tuned_fp,\n",
    "                                                   knn_tuned_fn,knn_tuned_tp),\n",
    "                           'Variable Option'    : 'option 1',\n",
    "                           'Variables'           : values_list[0]},ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-montreal",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "genuine-personal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:45:42.957424Z",
     "start_time": "2021-02-15T21:45:42.702459Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "Training ACCURACY: 0.9993\n",
      "Testing  ACCURACY: 0.731\n",
      "ACCURACY GAP     : 0.2683\n",
      "AUC Score        : 0.6259\n",
      "\n",
      "Confusion Matrix\n",
      "True Negatives : 52\n",
      "False Positives: 104\n",
      "False Negatives: 27\n",
      "True Positives : 304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST  \n",
    "\n",
    "random_forest_data1   =  ap_customer_2.loc[ : , options['option3']]\n",
    "target  =  ap_customer_2.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            random_forest_data1,\n",
    "            target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = target)\n",
    "\n",
    "\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 219)\n",
    "\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "rf_default_train_score = rf_default_fit.score(x_train, y_train).round(4)\n",
    "rf_default_test_score = rf_default_fit.score(x_test, y_test).round(4)\n",
    "rf_default_AUC_score = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "rf_default_tn, \\\n",
    "rf_default_fp, \\\n",
    "rf_default_fn, \\\n",
    "rf_default_tp = confusion_matrix(y_true = y_test, y_pred = rf_default_fit_pred).ravel()\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print(\"Results\")\n",
    "print('Training ACCURACY:', rf_default_train_score)\n",
    "print('Testing  ACCURACY:', rf_default_test_score)\n",
    "print('ACCURACY GAP     :', rf_default_train_score - rf_default_test_score)\n",
    "print('AUC Score        :', rf_default_AUC_score)\n",
    "      \n",
    "print(f\"\"\"\n",
    "Confusion Matrix\n",
    "True Negatives : {rf_default_tn}\n",
    "False Positives: {rf_default_fp}\n",
    "False Negatives: {rf_default_fn}\n",
    "True Positives : {rf_default_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greenhouse-printer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:46:18.423386Z",
     "start_time": "2021-02-15T21:45:42.960201Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  app.launch_new_instance()\n",
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuned best estimors:\n",
      "Tuned Parameters  :\", {'warm_start': True, 'n_estimators': 350, 'min_samples_leaf': 1, 'max_depth': 8, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Tuned Training AUC:\", 0.61\n",
      "\n",
      "\n",
      "Tuned Random Forest Results:\n",
      "Training ACCURACY:  0.8149\n",
      "Testing  ACCURACY:  0.8152\n",
      "ACCURACY GAP:       -0.000300000000000078\n",
      "AUC Score:'         0.7234\n",
      "\n",
      "\n",
      "Tuned Random Forest Confusion Matrix:\n",
      "True Negatives : 73\n",
      "False Positives: 83\n",
      "False Negatives: 7\n",
      "True Positives : 324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuned RANDOM FOREST  \n",
    "rf_train_acc = rf_default_fit.score(x_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(x_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "leaf_space       = pd.np.arange(1, 31, 1)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "depth            = pd.np.arange(1, 9, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space,\n",
    "              'max_depth'        : depth,}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "forest_cv_ = RandomizedSearchCV(estimator           = forest_grid,\n",
    "                               param_distributions = param_grid,\n",
    "                               cv         = 3, \n",
    "                               n_iter     = 120, \n",
    "                               random_state = 219,\n",
    "                               n_jobs = -1,\n",
    "                               scoring    = make_scorer(roc_auc_score,\n",
    "                                            needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "forest_cv_.fit(random_forest_data1, target)\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "Tuned best estimors:\n",
    "Tuned Parameters  :\", {forest_cv_.best_params_}\n",
    "Tuned Training AUC:\", {forest_cv_.best_score_.round(4)}\"\"\")\n",
    "\n",
    "# 8. TUNED RANDOM FOREST Model OPTION 2 \n",
    "forest_tuned_ = forest_cv_.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred_ = forest_tuned_.predict(x_test)\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned_.score(x_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned_.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred_).round(4) # auc\n",
    "\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "rf_tuned_tn, \\\n",
    "rf_tuned_fp, \\\n",
    "rf_tuned_fn, \\\n",
    "rf_tuned_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred_).ravel()\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "Tuned Random Forest Results:\n",
    "Training ACCURACY:  {forest_tuned_train_score}\n",
    "Testing  ACCURACY:  {forest_tuned_test_score }\n",
    "ACCURACY GAP:       {forest_tuned_train_score - forest_tuned_test_score}\n",
    "AUC Score:'         {forest_tuned_auc}\n",
    "\n",
    "\n",
    "Tuned Random Forest Confusion Matrix:\n",
    "True Negatives : {rf_tuned_tn}\n",
    "False Positives: {rf_tuned_fp}\n",
    "False Negatives: {rf_tuned_fn}\n",
    "True Positives : {rf_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stone-committee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:46:18.436475Z",
     "start_time": "2021-02-15T21:46:18.424890Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adding Models for overview \n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Random Forest',\n",
    "                           'Training Accuracy'  : rf_default_train_score,\n",
    "                           'Testing Accuracy'   : rf_default_test_score,\n",
    "                           'AUC Score'          : rf_default_AUC_score,\n",
    "                           'Confusion Matrix'   : (rf_default_tn,\n",
    "                                                   rf_default_fp,\n",
    "                                                   rf_default_fn,\n",
    "                                                   rf_default_tp),\n",
    "                           'Variable Option'    : 'option 3',\n",
    "                           'Variables'           : values_list[2]},ignore_index = True)\n",
    "\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned Random Forest',\n",
    "                           'Training Accuracy'  : forest_tuned_train_score,\n",
    "                           'Testing Accuracy'   : forest_tuned_train_score,\n",
    "                           'AUC Score'          : forest_tuned_auc,\n",
    "                           'Confusion Matrix'   : (rf_tuned_tn,\n",
    "                                                   rf_tuned_fp,\n",
    "                                                   rf_tuned_fn,\n",
    "                                                   rf_tuned_tp),\n",
    "                           'Variable Option'    : 'option 3',\n",
    "                           'Variables'           : values_list[2]},ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-eugene",
   "metadata": {},
   "source": [
    "### Gradient Boosted Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "powered-radiation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:46:38.334381Z",
     "start_time": "2021-02-15T21:46:18.439665Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/estrellaspaans/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned best estimors:\n",
      "Tuned Parameters  : {'warm_start': True, 'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.7000000000000001}\n",
      "Tuned Training AUC: 0.5299\n",
      "\n",
      "Training ACCURACY :   0.8986\n",
      "Testing  ACCURACY :   0.883\n",
      "ACCURACY GAP      :   0.015599999999999947\n",
      "AUC Score         :   0.8258 \n",
      "\n",
      "Tuned Random Forest Confusion Matrix:\n",
      "True Negatives : 104\n",
      "False Positives: 52\n",
      "False Negatives: 5\n",
      "True Positives : 326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuned Gradient Boosted Machines \n",
    "gbm_data =  ap_customer_2.loc[ : , options['option4']]\n",
    "target = ap_customer_2.loc[ :,\"cross_sell_success\"]\n",
    "\n",
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            gbm_data,\n",
    "            target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = target)\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "learn_space        = pd.np.arange(0.1, 1.0, 0.2)\n",
    "estimator_space    = pd.np.arange(100, 300, 25)\n",
    "depth_space        = pd.np.arange(1, 8, 1)\n",
    "warm_start_space   = [True, False]\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'learning_rate' : learn_space,\n",
    "              'max_depth'     : depth_space,\n",
    "              'n_estimators'  : estimator_space,\n",
    "              'warm_start'     : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "full_gbm_cv = RandomizedSearchCV(estimator     = full_gbm_grid,\n",
    "                           param_distributions = param_grid,\n",
    "                           cv                  = 3,\n",
    "                           n_iter              = 100,\n",
    "                           random_state        = 219,\n",
    "                           n_jobs              = -1,\n",
    "                           scoring             = make_scorer(roc_auc_score,\n",
    "                                                 needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "full_gbm_cv.fit(gbm_data,target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned best estimors:\")\n",
    "print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))\n",
    "\n",
    "\n",
    "# INSTANTIATING with best_estimator\n",
    "gbm_tuned = full_gbm_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned.predict(x_test)\n",
    "\n",
    "\n",
    "gbm_tuned_training_score = gbm_tuned.score(x_train, y_train).round(4)\n",
    "gbm_tuned_testing_score = gbm_tuned.score(x_test, y_test).round(4)\n",
    "gbm_tuned_auc_score = roc_auc_score(y_true  = y_test,y_score = gbm_tuned_pred).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "# SCORING the results\n",
    "print(f\"\"\"\n",
    "Training ACCURACY :   {gbm_tuned_training_score}\n",
    "Testing  ACCURACY :   {gbm_tuned_testing_score}\n",
    "ACCURACY GAP      :   {gbm_tuned_training_score - gbm_tuned_testing_score}\n",
    "AUC Score         :   {gbm_tuned_auc_score} \n",
    "\n",
    "Tuned Random Forest Confusion Matrix:\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "american-cross",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:46:38.342102Z",
     "start_time": "2021-02-15T21:46:38.335845Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Adding Models for overview \n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned GBM FINAL',\n",
    "                           'Training Accuracy'  : gbm_tuned_training_score,\n",
    "                           'Testing Accuracy'   : gbm_tuned_testing_score,\n",
    "                           'AUC Score'          : gbm_tuned_auc_score,\n",
    "                           'Confusion Matrix'   : (gbm_tuned_tn,\n",
    "                                                   gbm_tuned_fp,\n",
    "                                                   gbm_tuned_fn,\n",
    "                                                   gbm_tuned_tp),\n",
    "                           'Variable Option'    : 'option 4',\n",
    "                           'Variables'           : values_list[3]},ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-picnic",
   "metadata": {},
   "source": [
    "## 6. All Models & Run Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "republican-secret",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:46:38.350301Z",
     "start_time": "2021-02-15T21:46:38.343561Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL PERFORMANCE: \n",
      "\n",
      "                   Model Name  AUC Score  Training Accuracy  Testing Accuracy    Confusion Matrix Variable Option\n",
      "0         Logistic Regression     0.6391             0.7252            0.7351   (58, 98, 31, 300)        option 1\n",
      "1               Decision Tree     0.6119             0.7293            0.6982   (58, 98, 49, 282)        option 1\n",
      "2         Tuned Decision Tree     0.6779             0.7430            0.7556   (72, 84, 35, 296)        option 1\n",
      "3  TUNED KNN Non Standardized     0.6119             0.7512            0.7639   (65, 91, 24, 307)        option 1\n",
      "4               Random Forest     0.6259             0.9993            0.7310  (52, 104, 27, 304)        option 3\n",
      "5         Tuned Random Forest     0.7234             0.8149            0.8149    (73, 83, 7, 324)        option 3\n",
      "6             Tuned GBM FINAL     0.8258             0.8986            0.8830   (104, 52, 5, 326)        option 4 \n",
      "\n",
      "OPTION 1 VARIABLES: \n",
      "['mobile_number', 'cancellations_before_noon', 'pc_logins', 'mobile_logins', 'refrigerated_locker', 'work', 'personal', 'other', 'female', 'male', 'unknown', 'tastes_and_preferences', 'contacts_w_customer_service', 'product_categories_viewed', 'has_early_deliveries']\n",
      "\n",
      "OPTION 3 VARIABLES: \n",
      "['mobile_number', 'cancellations_before_noon', 'total_logins', 'refrigerated_locker', 'personal', 'other', 'tastes_and_preferences', 'contacts_w_customer_service', 'product_categories_viewed', 'has_early_deliveries', 'female', 'male', 'total_orders', 'has_master_class_attended', '60_min_prep_time', 'total_meals_ordered']\n",
      "\n",
      "OPTION 4 VARIABLES: \n",
      "['revenue', 'total_meals_ordered', 'unique_meals_purch', 'contacts_w_customer_service', 'product_categories_viewed', 'avg_time_per_site_visit', 'mobile_number', 'tastes_and_preferences', 'pc_logins', 'mobile_logins', 'weekly_plan', 'late_deliveries', 'average_meals_ordered', 'total_photos_viewed', 'personal', 'female', 'has_cancellations', 'locker', 'click_above_avg_10', 'high_low_ranking', '60_min_prep_time', 'has_master_class_attended', 'has_early_deliveries']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing All models together \n",
    "print(f\"\"\"\n",
    "MODEL PERFORMANCE: \n",
    "\n",
    "{model_performance.iloc[:,0:6]} \n",
    "\n",
    "OPTION 1 VARIABLES: \n",
    "{model_performance.iloc[0,-1]}\n",
    "\n",
    "OPTION 3 VARIABLES: \n",
    "{model_performance.iloc[4,-1]}\n",
    "\n",
    "OPTION 4 VARIABLES: \n",
    "{model_performance.iloc[6,-1]}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tested-tenant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:46:38.358880Z",
     "start_time": "2021-02-15T21:46:38.356492Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seconds to run: 75.97\n"
     ]
    }
   ],
   "source": [
    "#Printing Time to Run \n",
    "end = time.time()\n",
    "print(f\"\"\"\n",
    "Seconds to run: {round((end - start),2)}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
